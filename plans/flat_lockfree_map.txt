
flat_lockfree_map

Flat:
|  Use only contiguous memory so that the serialized version is the normal version

Concurrent:
|  Lock free except when resizing
|  Do copy on write so that the reads don't stop 
|  Make mutex wait time for writes minimized
|

Hashing Insights:
|  With robin hood hashing multiple entries from the same key will be next to each other in memory
|  |  One data structure can be used for an unordered multi-map
|  Something about the fact that Ids are unique due to a reader keeping blocks from deletion, so versions aren't needed 
|  |  32 bit block indices can be used, they can be atomically swapped, and thus robin hood hashed


Reader Count:
|  Reader count needs to be incremented on two numbers before they are swapped
|  Use a deleted flag in an atomic struct and u32 : 31 for reader count
|  |  No, deleted doesn't need a flag, it just needs a special value


Template Arguments:
|  Use a variable template argument so that a arbitrary tuple can be returned?
|  |  How does a template allocator argument work if variable template arguments are used for values?
|  |  The problem is that you would mostly want the allocator to be an argument with a default value of the default allocator
|  |  Could the template argument be layed out as <key, value, allocator=default, values...>
|  |  Could the template arguments be side stepped by using a static make_ function instead of constructors
|


Return Values:
|  Use std::optional<> with operator[] ?
|  Use a custom struct that has operator bool() ?
|  |  Could have extra information such as how many values are in this key
|  Use operator() to get a specific value out of a key + tuple


Make untyped container with void* so that template bloat is minimal
|  Variable holds size in bytes
Use std::hash() ?
Save hash near reader count and pack them all together
Use power of 2 sizing


Does the number of readers need to be part of the block index in the new atomic lookup array?
|  If you lookup a block index, how do you know you haven't had it deleted from you?
|  Use 8 bits for readers, 24 bits for the index, and two special values for EMPTY and DELETED
|  Make START_SPECIALS = DELETED
|  Last thread out frees the block
|  |  Thread that sees readers == 1 && DELETED frees the block by setting the readers to 0 and the index to EMPTY 
|  |  thread should see readers == 0 and DELETED



If two indices are being swapped atomically, do they need the readers attached to them?
|  They couldn't be changed to DELETED or EMPTY underneath
|  They could however be changed to be completely different values with the same indices
|  While the swap could be done atomically, you don't know if that is what you want to do
|  |  Maybe there is a way for eventual consistency by checking before and after two indices are swapped
|  |  However this introduces the possibility of an inserted value blinking out of existence due to another thread seeing an EMPTY or DELETED slot too early
|  This means that the actual sequence is:
|  |  Readers need to be incremented
|  |  The hashes need to be checked to make sure that their positions need to be swapped
|  |  Swap the indices
|  |  Decrement the readers
|  This is all neccesary because an insertion needs to start from the first free slot and swap backwards until:
|  |  The hash lines up with it's natural slot (hash % slots) or ...
|  |  The element at one index less has the same hash
|  |  |  What if they have the same hash but not the same key? 
|  |  |  Whole other can of worms, maybe they should be compared with operator< etc.
|  


Value meta data:
|  Store the hash
|  Store the index in the meta data? - no need since you are literally reading from the index when you read the meta data - you have to have it saved as a variable on the stack locally already
|  Hash and what else since the size is constant?  
|  Does the hash need to be stored? 
|  |  Storing the hash could save time since robin hood hashing will be doing comparisons with the neighbor slots
|  Does the meta data need to be saved in a separate place?
|  Why was it done like that previously? Cache locality?
|  Should the hash and the key be stored together with the value stored separatly?

What happens if an index is deleted, cleaned up, then the same index is used again and moved to a different slot while another thread is reading?
|  If a Idx is moved around, it can still be searched for
|  If the readers is incremented, that val_idx can't be used again, so you know it is unique
|  Even if it is moved, you know that the key it contains can't change and thus the hash can't change
|  If the hash can't change, you can start from the natural/ideal slot position and search forward until you find the index you are looking for
|

Possibly need to make sure that when deleting a Idx in a slot, DELETED is set atomically, then separately the readers are decremented, and only if the deleting thread gets back readers == 1 and DELETED is it responsible for freeing the block
|  If readers == 1 and DELETED the Idx needs to be successfully compare and swapped for readers == 0 and DELETED before freeing the block
|  Need to increment the readers before marking for deletion, so that there isn't confusion about what thread needs to delete 
|  |  This sets up the case where a thread can get be the only one to comapre and swap from readers==1 && DELETED to readers==0 && DELETED
|


How to iterate through multiple values for a single key?
|  On step forward do multiple steps
|  |  Increment the reader count
|  |  Read the key and value into the iterator
|  |  Decrement the reader count
|  |  On dereference, return the value that was already read
|  |  If no value is found, set the iterator to == end()
|


Have to make room for the block list
|  A block list header
|  A u32 for each index
|  Should the current list index eventually be padded to be on its own cache line to negate false sharing?
| 


Capacity and the concurrent list
|  Might need to keep track of capacity atomically with the concurrent list
|  Instead of version in the head, a u32 can be used for capacity 
|  Keeping track of capacity is important so that a resize can be done before reaching the end of the list

Freeing a linked list of indices all at once
|  There might not be an oportunity to free a linked list of indices instead of a single index
|  If there is a small thread local cache of indices, using a bulk free could become more plausible
|  


Concurrent List
|  Does the concurrent list need to keep the version number 
|  ABA problem exists in the form of the index under the compare and swap being the same, but the index in the cell/slot it references has changed
|  If 24 bits are all that is needed for the indices, at least the versions can have 8 more bits
|  24 bits for idx, 40 bits for version
|  40 bits gives a little over 1 trillion versions
|

Using as unordered_set, unordered_map, and unordered_multimap
|  Give values as a variadic template
|  Let the variadic type arguments that make up values be zero or more values
|  This will give a unified set, multi-set, map and multi-map
|  All with a multiple types, creating a table
|  Still not sorted so can't query ranges
|  Can still only use 1 key
|  |  Could this be changed using a variadic template for keys?
|  |  How would construction work? Would it need to be compound?
|  |  Ordering could get tricky - maybe the first key would need to be primary and other keys would jump to index that only contains the real index to jump to
|  Does boost's nightmare multi-index map enable lookup using multiple different key types as keys?
|  |  Yes, it enables one or more indices

Simplifying construction
|  Use tuples as lists of types?
|  Make or copy a simple list of types 
|  Should the choice of multiple values be left up to the user since they can use a tuple<> themselves?
|  Would a make_flf_map function be more elegant for custom allocators?
|  How should arbitrary memory like shared memory mapped files work? anothe make_ function?
|  |  might need malloc and free 
|  |  don't need realloc, because resizing will make a second buffer so that reading is always available

Should 8 bytes be left after sizeBytes for a type number?
|  Would this help, even in the case of LAVA?


Keeping track of size:
|  Can the size be incremented atomically but separatly from the ListHead ?
|  It would be serial and eventually consistent, but not in lock step with the ListHead Index and version
|  ListHead is isolated and could potentially be 128 bits instead of 64 bits
|


operator[] and operator()
|  Will these need to return a smaller struct / iterator that does the actual insert on operator= assignment?
|  

Robin Hood Insertion
|  what if the span of ideal slots changes while inserting
|  |  while inserting into a slot/cell, 
|  |  probe forward and find an empty slot
|  |  compare and swap sucessfully
|  |  meanwhile the span of slots before it has changed, and now the slot being inserted ends a span of another key
|  |  will need to check the previous slot at the same time as the current slot to make sure that it is in the correct key span
|  |  |  this will mean incrementing the readers of both, 
|  |  |  then checking their values
|  |  |  then potentially swapping them
|  |  |  then decrementing their values
|  Can a deleted slot get swapped? 
|  |  only after the thread owning deletion sets it to DELETED and readers to 0
|  |  what is important is that the DELETED slot doesn't change to EMPTY until the readers are 0
|  |  because of this, a DELETED slot can't be moved, because any reading thread needs to come back to that specific index to decrement its readers
|  |  the thread that sets it to DELETED and readers==0 will then need to swap it forward until it finds an EMPTY slot
|  |  when the deleting thread finds the EMPTY slot as the next slot, it will try to compare and swap them both to EMPTY
|  Big win for robin hood hashing is that finding a key that is not in the table has a very early exit
|  
|

Alignment
|  Might need to make the header contain padding for alignment 
|  There could be an offset parameter after the other parameters that says where the per element memory should start
|  This could ensure that alignments such as cache line specific variables and 128 bit aligned atomic operations are in place  
|

Initial capacity
|  Should the initial capacity be 8 or 16?
|  It could be argued that there is no reason to use a hash table for fewer elements since they could be searched linearly
|


Insertion
|  Need to figure out what will have to happen in insertion
|  It is possible that the insertion start will be an EMPTY slot at the end of different span than the inserting key
|  It is possible that other indices closer to the ideal position will be moved around
|  Is it possible that the inserting Key will be moved by another thread before it is in its ideal position?
|  |  Yes, because DELETED slots/cells will get bubbled forward until they find an EMPTY slot
|  |  A thread can be inserting a key and find that the key is no longer where it put it
|  |  It will have to loop towards the ideal position to find it 
|  |  It can then keep trying to swap it towards it's ideal position, also stopping if it hits a Key that is further or the same distance from its ideal position

Find / Get
|  What are the conditions for stopping a find?
|  |  Can't stop on finding something further from its ideal position (further than what?)
|  |  Can stop if hitting an EMPTY slot/cell
|  |  Can stop if the current cell is closer to it's ideal position than the number of indices searched? 
|  |  If the slot being checked has an ideal position forward from the ideal position of the Key being searched for, that should mean they Key isn't found
|
|



