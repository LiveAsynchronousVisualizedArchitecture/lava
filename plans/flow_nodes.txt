

Should data flow nodes have a separate function from the main that runs first?
 - Gives the number of allocations along with each of their sizes
 - Then one thread local allocation could be done
 - Would be difficult for larger nodes to give the maximum amount of memory needed if they had the potential to re-use memory.


Is scatter needed?
 - The real synchronization should come from data
 - Ordering can be done at any point by sorting on the version number
 - any list should be able to be 'scattered' and processed by different threads because the data is separate


Make message passing nodes circles and data flow containers squares
 | both could contain stats
 | the message passing nodes could have segments on the border that show their connections/messages to data flow containers and other message nodes
 | | use an arc along the border and change it to hav a teardrop shaped protrusion that transitions into the line/noodle showing the connection?




Can there be a node that enables shared memory parallelism like openMP?
|  Need an init() method that looks at the input and allocates memory for the output 
|  |  Will this need to use global memory allocation like malloc, jemalloc, or rpmalloc?
|  Need a method to get the next section of memory to operate on, possibly just needs to increment an ID atomically
|  Should this only be a node that creates multiple packets that contain all that is needed for other nodes to use shared memory parallelism?
|  |  Would need to be an actual memory address to non-thread local memory
|  |  Need to have a flag saying that it is a memory address and not an index
|  |  Need to contain the global memory address, the ID for what section this packet is for and a way to reference count the global address so it can be freed?
|  |  Would want a tbl with it that would contain parameters for how to use the memory and the ID (like image dimensions)




Node Types
|  how to handle constant inputs
|  |  should each node have a potential tbl of constants?
|  |  this would mean the first argument could be a tbl/pointer to the table memory
|  |  when using the node graph, the table could be output as a .h file and compiled into the shared library 
|  |  the constants tbl could be passed into the node function or use an implicit pointer


Accumulator / General loop and iteration 
|  Should there be a specific accumulator node? 
|  Can accumulation of the output with new inputs be done with:
|  |  Nodes able to take optional arguments
|  |  A node that takes a list and an optional argument to inject into that list
|  |  The ability to make circular references in the graph
|  Does the ability to make circular references depend on either branching, optional arguments or both so that there is a way to break out of the loop? 
|  |  accumulation could be done with a branching node before the accumulator node 
|  |  the branch bypasses the accumulator if there is only one packet in the list
|  |  if there is more than one packet, the branch goes to the accumulator
|  |  the accumulator's output would go back in to the branch   -------the branch node's single packet path and the accumulator's output path go to some node with two optional inputs - could even be another accumulator node
|  |  the branch would need two inputs to collect all the packets - if there is only one it bypasses the accumulator, if there are multiple, it feeds them into the accumulator


Ordering
|  Should all data packets be dealt with before going back to the message nodes?
|  Should it be done in an interlaced way to keep message passing nodes from lagging?
|  Should there be generator nodes and message passing nodes, or just message nodes?
|  Does the frame number need to be specific to each message node? Instead of frame number, a frame + msg node combination


Concurrency with flow node types
|  Revisit original node type ideas and see how they line up with the current packet method
|  If there are lists being output, does a dedicated split node need to exist?
|  If every input can potentially be a list is a gather node needed? 
|  If a split can be automatically parallized, that could imply that all each node is run with only one packet from a list
|  The flip side is entire lists being passed in automatically and splitting/parallelism done manually
|  Automatic parallelism whenever it is possible may be a better bet than expecting anyone to manually choose a special split node or split node attribute. 
|  A special gather node or gather node attribute to combine a list seems more reasonable because it makes parallelism automatic and the exceptional case of a gather node neccesary 
|  If a gather node is the manual route neccesary, tools and error messages can help the learning curve. 
|  Will message nodes need to take lists in their entirety? 
|  Should there be something in between a full gather and a node that only takes single packets? 
|  A "Take" node that can take as many or as few packets for each slot as it wants?  First call returns the number of packets the Take node wants.

Concurrency with data structures inside LavaFlow 
|  Likely will need a resizable array of atomic variables
|  Look back at atomic vector / atomic array and make it double buffered so that it can be resized without locking


Should the function passed be one that inserts a buffer instead of a simple allocation function?
|  The decision to place the output in memory would then be done in the flow loop
|  The output would then be placed directly into the db instead of copied to memory and copied to the db
|  If the output function needs to copy straight from a buffer anyway, would that happen no matter what? 
|  What happens with Cereal?
|  |  Cereal uses an archive object so a custom archive object could concievably be created
|  |  However it seems that likely there would be a serialization step and a copying step
|  |  A function that could potentially copy straight into the db would still save a copy
|  Should there be a tbl flatten function that 
|  Should a lava alloc function and allocator always be used so that if the db isn't needed there is potentially no copy?
|  |  does this take it back to the same place of always allocating memory and letting the loop store in the db if neccesary?
|  Make sure that references start at 0 for memory allocated with the lava_alloc function 
|  |  this will mean that memory allocated with it will be thread local and 'garbage collected' at the end of the loop, right after the node is done executing 
|  |  this should also mean that even nodes written in C don't need to worry about manual memory deallocation
|  |  could be used for C or ISPC


Visualization and internal allocations
|  Add the ability to visualize all the lava_alloc allocations that happened in a node 
|  |  Create the option to visualize a node instead of just slots
|  |  Initially check the lava allocations for 't' and 'b' magic numbers at the start
|  Eventually look for "IdxVerts" in the type, along with "Spread" and or string points / labels points
|  Register shared libraries as conversions from user made types to the types that can be visualized
|  |  Show graphically that slots can or can't be visualized (determined by their type being one of the visualizer types or a conversion to a visualizer type being known)
|  |  Register in the json file?
|  |  Register in the plugin struct as a converter? 
|  |  Should there be explicit nodes to convert to a visualizable type, or will that end up ultimatly being too many nodes that create visual clutter, since ideally every slot would be visualizable
|


Dealing with frames of packets
|  Does every framed instance need an atomic frame array that contains packets? 
|  |  Thread gets a packet - doesn't increment it's reference since that has already been done 
|  |  Checks the node to see if this packet completes a frame
|  |  |  If not, atomically copy the packet into the instance's frame array 
|  |  |  If it does complete the frame, then run this thread is the one to run the node with the current frame 
|  |  |  Have to make sure that frames run sequentially and a later frame doesn't run before a younger frame
|  |  Take a packet out of the queue, if it can't be run due to being a frame that's too new, take the next packet out and put the previous blocked packet back in
|  Should there be a packet frame queue whose size is a minimum of the number of threads and the number of nodes?
|  Should the packet queue be the size (or roughly the size) of the number of nodes?
|  Instead of packet queue have a simple ring buffer?  Or concurrent hash of packet frames? Can they be combined?
|  Linear search or hash search for node + frame combination, then put the packet in the frame
|  Make a priority queue that contains frames 
|  |  sort by inverse of number of slots left
|  |  Sorted by frame numbers
|  |  Then sorted by nodes
|  |  Linear search the frames for a match
|  |  Insert packet into the frame
|  |  Have to re-sort somehow - either take the frame out and put it back in, or resort the queue in place - maybe not if frames aren't sorted by how many free packet slots they have
|  |  Does unframed data have to be handled somehow?  A frame number constant that represents any frame?
|  Does each node need to take a pointer to a single frame instead of an array of packets?


Errors:
|  Does a packet or frame need to have a special error mode? 
|  |  A frame could have an error type and contain what node and frame number it errored on
|  |  Should an error frame, be a copy of the frame that was used for input?
|  Erlang has a Outcome type that is a union of return type and error


Range:
|  Should each packet contain a start and end range
|  Should there be a flag that says whether the range is used or should the ranges have high value constants to indicate they are not used?
|  Is just one range enough to let the next node interpret the range however it needs to ?
|


Packet Queue by cache and memory heirarchy:
|  Does this imply setting thread affinities to specific cores?
|  Packet queueing based on cache and memory heirarchies 
|  |  look in a queue for the physical core, then whatever shares the L2 cache, L3 cache, and finally the same NUMA node 
|  |  special queue structure needed that will automatically use this heirarchy by being specifically structured around it 
|  Need to weigh importance of having the absolute correct ordering versus using the memory heirarchy as effectivly as possible 
|  |  maybe the memory heirarchy is the most important, but would out of order frames cause problems? 
|  |  possibly only if message passing nodes did not make sure to process the frames they recieve in order 
|  Would this end up being a tree structure of queues?
|  |  Could it even be a 2 dimensional tree structure? 
|  |  |  One dimension is the memory heirarchy, one dimension is the priority of packets
|  |  |  Top level in X - buckets representing NUMA nodes (if there are more than one)
|  |  |  Second level in X - buckets representing different L3 caches (if there are more than one) 
|  |  |  Third level in X - buckets reprsenting different L2 caches
|  |  |  Fourth level in X - buckets reprsenting different physical cores (which should share L1 cache, maybe other resources as well)
|  |  Does splitting up the packets in Y like a kd-tree even make sense? 
|  |  |  While it may be easier to think about packets bubbling up the memory heirarchy, but it is probably a much better fit to think of a thread with no more packets in its queue to go up the memory heirarchy 
|  |  |  While 'going up the memory heirarchy' a thread looking for a packet is actually going to end up searching into many small queues of the lowest level in the heirarchy
|  |  |  Is there an elegant solution to be to looking into higher levels of the memory heirarchy? 
|  |  |  |  When a thread searches up in the memory heirarchy, should it dequeue packets from each of the sub-queues into a higher level queue?
|  |  |  |  Does this imply that the higher level packets should come first since they would have been taken from the top of the lower level queues as well as being sorted against each other? - seems reasonable to treat it that way
|  |  |  |  This implies that higher level packets will be processed only after some thread finishes all its most memory local work 
|  |  |  |  Is the fundamental principle a stack of queues? Is there anything novel about thinking of it like that or is it simply an inverted version of a tree?


2 CPU NUMA with 2 cores 4 threads hypothetical: 
   ___________        Main memory
   _____|_____        NUMA nodes
   __|__|__|__        L3 slices or L2 caches per physical core - both hyper threads should share the same cache and thus don't need individual queues



Message Nodes and frame incrementing:
|  Need to find a single message node and run that on each loop
|  Make this mechanism integrate with the frame number so that all message nodes get a chance to run on a specific frame number before advancing to a new frame number?
|  How to resolve the fact that message nodes will get data packets with the idea that they should all be in sync with respect to each other's frame numbers?
|  |  Should each message node have it's own frame number that has no bearing on the other message node's frame numbers?
|  |  Should there be local frame numbers as well as a global frame number? 
|  |  How can two message nodes hook their outputs into a single flow node's two inputs? 
|  |  Should this even be allowed? - if it isn't it would introduce a potentially awkward situation where data has to be passed through a message node so that that message node can output two packets as two arguments to a data flow node
|  |  Very likely should be allowed, but what does that imply in terms of the underlying structure of syncing arguments with frames? 
|  |  |  Does this imply that every node that needs it's own frame number?  
|  |  |  Are there really 3 levels of heirarchy to a LavaId? frame number, node id, slot index 
|  |  |  The node instance's current frame number would need to be atomic and incremented when creating a LavaFrame
|  Does a frame number per node instance imply that frames should be sorted so that the lowest frame number runs first? (Which was already the case)
|  |  Would this mean that it prioritizes packets that are further down the pipeline? - that seems ideal to minimize the number of packets in flight
|  |  Does this imply that node instances that are more rarely run are prioritized? If so, is that good, bad or a double edged sword?  
|  Does a packet still need a frame number? 
|  |  Probably yes, since it represents the frame number of the source node
|  |  Does this pose any sort of synchronization problems? Can a packet from a later frame get ahead in the queue? 
|  |  |  Probably depends on how the the queue is built
|  |  |  If the queue is based on atomic actions, it may be possible to make sure that the head of the queue/heap is always correctly sorted even if other parts are only eventually consistent 
|  |  |  Might bring back the idea of some sort of b-tree or a heap that feeds into a small insertion sorted list 
|  How to distribute fair scheduling of message passing nodes with minimum synchronization?
|  |  First approach can be to simply have a global next message node index
|  |  Maybe that is enough, especially if it is padded to its own 64 byte cache line so that any contention doesn't produce false sharing to other atomic variables
|



Gather Nodes:
|  Ideally, would there be a mechanic where multiple packets of the same frame, node and socket create a list that is passed instead of a single packet? 
|  |  Would this decrease concurrency? 
|  |  Could be treated normally as separate packets unless encoutering a gather node, which could be passed as many packets of a frame as possible
|  Need as many examples of gather nodes as possible to understand the problems it would offer solutions to
|  |  Accumulating tiles together while rendering 
|  |  |  A gather node takes as many packets of tiles as it can
|  |  |  Accumulates them into one packet per tile
|  |  |  Passes those through to a frame buffer message passing node that keeps the state
|



