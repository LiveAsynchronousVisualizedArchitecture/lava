

messages need to contain:
|  Either an index into the db OR an address in process memory space
|  A type
|  The size - If the size is known, length queries are not needed and allocations can be done in bulk - one allocation to hold all the incoming data from the db
|  |  How to rectify this with some indices being in process memory already?
|  |  Loop through inputs and add up all the sizes from db indices
|  |  Loop through again, changing the indices to pointers to addresses in the bulk memory allocation
|  |  Copy from the database for all the indices (need to hang on to the indices somehow)

sending messages means:
|  Incrementing the references for an index / address
|  Decrementing references must happen by the thread after it is finished with the incoming data


is there value in a message passing node optionally being a state machine 
|  constructed visually
|  more constraints 
|  more checks


Different types of message nodes:
|  First is a message node that has an atomic bool that lets the Lava loop skip it while another thread is executing it
|  Second is a message node that has its own thread, and the running thread hands off to that thread using a mutex
|  |  Should this be combined with the first node type?
|  |  What would be the implications? 
|  |  If this was the default type, things like a dedicated openGL message node would be easy 
|  |  Since every message node would have it's own thread, all openGL commands would be run from that thread
|  Third type is a message node that is thread safe and can be executed by multiple threads concurrently   
|  |  The main building block would be having a state made out of thread safe data structures 
|  |  Writes shouldn't block reads 
|  |  Reads shoudln't block writes from happening - multi-buffered copy on write structures could work in a general sense
|  |  Might need some dedicated lock-free structures to make this practical for most people
|  |  Single header file and flat memory respresentation would be ideal
|  |  |  Robin Hood Hash map?
|  |  |  lock-free tbl?
|  |  |  Copy on write vector?
|  |  Simdb could be used as a general data store 
|  |  |  Does this open up another can of worms such as a simdb file per state?
|  |  |  If types stored in a message node's state are a visualizable type, then maybe the visualizer could be used to peek into the state
|  |  |  Types that are only convertable to a visualized type would need to be explicitly converted by the Lava interface / Fissure GUI
|  |  |  Could be a control on the message node
|  |  |  Reminicent of the idea of being able to peek into any allocation done with lavaAlloc inside of a flow node
|


Should Message Nodes have their own input queues?
|  This would allow message nodes to elegantly use all input packets available to them
|  Maybe the node could then do whatever it wants with all the inputs in the queue
|  The return value could indicate whether the LavaLoop should treat this as being executed, or it could indicate how many input packets were used
|  The message node template could have its own thread and mutex embedded in it to stop the calling thread and start its own, which makes the message node is thread safe and always run by the same thread by default
|  Removing the template thread and mutex would mean that the state needs to be made thread safe by the user
|  Would a node having its own controllable queue as opposed to being given single packets (as well as being run every cycle) be the most fundamental way to separate transformations from state management?  
|  |  If a node can manage its own inputs and is run every cycle, does this give back the control and decision making lost in data flow nodes?
|  Should there just be an explicit lock free gather node? 
|  |  Give a lock free gather a similiar look to flow nodes? 


Should message nodes have a different name?
|  Gather nodes
|  Sync or Synchronize nodes
|  State nodes 
|  Actor nodes
|  Core node
|  Synchronization nodes make sense because if you take in multiple packets, you are inherently synchronizing them 
|  State nodes makes sense if there is state being saved, though that may not be the case if a node is just combining packets 
|  Gather nodes makes sense if a node is combining packets and possibly if the node is saving state - gather doesn't make sense if the node is being used without inputs, though if there are no inputs, wouldn't a flow node with no inputs / generator node work the same way? - if there is a locking mechanism then that could make a difference, even with no inputs - would gather still make sense?
|  Actor nodes use a known name, though that is probably the only name advantage as it doesn't describe the fundamentals of what they do
|  Core node indicates they are central points, though not much else about their name either
|  Does 'message passing' as a node name make any sense? 


Pipelining Message Nodes
|  If incrementally combining packets, it should be possible to output to a flow node, which then loops to the gather node's input 
|  |  This would only be able to combine a static number of packets together in the flow node due to the static limits on input packets, though maybe that is ok
|  Does a queue to a message node need to be able to transactionally take a number of packets? 
|  |  If the goal is to combine packets and the node is not being used in a single threaded manner, two threads could each take a single packet, then put it back in the queue when they don't get a second one, creating threads looping due to competing with each other 
|  |  It may be enough to query how many packets are available, then use an atomic in the node to keep a count of the packets intended to be taken, which could minimize contention
|  |  Leaving this up to a user to both discover and execute correctly would probably mean it would rarely ever be done and done correctly - wouldn't follow the purpose of effortless high performance concurrency 
|  

Should There be multiple templates?
|  One for flow nodes
|  One for message / gather nodes?
|  One for lock free gather nodes?


Multiple outputs linked to a single input
|  Flow nodes can't have multiple connections to a single input because they need to have a single packet in each input slot 
|  Gather nodes could possibly have multiple connections of the same type going into their queue to avoid needing to use multiple inputs
|  Might not be worth the complexity


How do program terminate?
|  Have a special exit/terminate output type or value?
|  Have a special exit/terminate packet type?
|  When an exit packet is seen, a global flag could be set so that nodes don't cycle any more 
|  Might need to make sure that all generators are run the same amount of times
|  What other edge cases are there? Do nodes need to know that an exit packet has been seen?


