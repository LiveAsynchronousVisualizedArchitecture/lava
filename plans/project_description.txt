



LAVA is a series of simple, modular, C++11 parts that combine to form an architecture and tools combination.   It enables putting together software using native languages in a way that makes iterations faster, intermediary results more transparent, and concurrency happen naturally.

There are three main elements of the architecture - message passing nodes, data flow nodes, and a lock free data store. A node (message or data flow) is made using a shared library. The message passing nodes control high level logic and can communicate with each other and/or transform data by calling a data flow network. The program is executed by threads entering a loop that executes message nodes when they have messages available and data flow nodes when they have data available. 

The tools are the node graph interface, visualizer and tbl data structure. The interface allows building networks and controlling what intermediary data is visualized.  The visualizer is a separate process that maps the shared memory key value store to view the actual data in the program as it executes.  Tbl.hpp is a single header vector+hash map data structure. It is always represented by flat continguous memory and so has no separate serialzation. Becase it can contains arrays, key-number combinations and sub tables, as well as being used directly from it's flat respresenation without a deserialization step, it is ideal for quickly creating data structures that can be used to communicate between nodes.  

I think the ramifications for AI research are enormous. When dealing with large amounts of data, I feel the ability to visualize it in any way is crucial to quickly understanding it and how it is being transformed. The ability to rapidly iterate as well as define truly modular pieces opens the possibility of real component reuse. Native languages and tools like C++11 mean existing libraries can be leveraged and programs can be fast and free of large dependencies. The lock free concurrent way that nodes are run means that every core can be fully utilized with little extra effort.  

Part of the idea came from raytraced rendering research. I was creating a kd-tree with arbitrary dimensionality and hit bugs I felt were too difficult to fix without seeing the kd-tree visually.  The first version paused and displayed the kd-tree in an openGL window. The second used a separate thread to watch the kd-tree being constructed in real time. Later a third was a separate program that communicated over local IP and eventually, shared memory.  Now I've written my own shared memory key value store (simdb.hpp) that is one file, has 100% lock free concurrency, is cross platform, Apache 2 licensed and can do hundreds of thousands of small reads and writes per core.

The other aspect came from experience with tools such as Houdini, Nuke and Touch Designer.  These all have data flow node based workflows which work well in their respective contexts. Since the inputs and outputs of every node can be visualized, problems can be narrowed down quickly. Inputs can be frozen so that a single node can be iterated rapidly with the same data passed through and visualized on every change.  With the use of message passing nodes high level logic and decisions can be introduced elegantly, solving a major achilles heel of data flow workflows.  To quote Sean Parent "all systems eventually grow and turn into a network problem"  

For AI, enabling the ability to build with modularity, iterate quickly and understand visually is something I think is worth working towards. Many of the pieces are already in place to an extent while others exist in a rough form or in previous versions.









LAVA is a series of simple, modular, C++11 parts that combine to form an architecture and tools combination.  new way to create software. It enables putting together software using isolated pieces made with existing languages and tools in such a way that iterations are faster, development is more transparent, and the resulting programs effortlessly use all available concurrency. 




There are three main elements of the architecture - message passing nodes, data flow nodes, and a lock free data store.  The big picture is to create C++11 programs using high level message passing and data flow nodes that communicate using indices into a lock free key value store.  Threads only need to be created once and all take serialized chunks of data from a lock free queue and run them with their destination node. The nodes would be non-trivial native shared libraries that result in a native program without dependencies.  



Part of the idea came from raytraced rendering research. I was creating a kd-tree with arbitrary dimensionality and hit bugs I felt were too difficult to fix without seeing the kd-tree visually.  The first version paused the program and displayed the kd-tree in an openGL window. The second version was able to use a separate thread to watch the kd-tree being constructed in real time. The third was a separate program that communicated over local IP, and later, shared memory.  Now I've written my own shared memory key value store (simdb.hpp) that is one file, has 100% lock free concurrency, is cross platform,  Apache 2 licensed and can do roughly 500,000 reads and write per core.




For AI, the ability to create real software that isn't limited to a sandbox or VM while enabling the ability to iterate quickly, build modularly and understand visually is something I think is worth working towards and completely obtainable.  Many of the pieces are already in place to an extent while others exist in a rough form or in previous versions.
