LAVA is a series of simple, modular, C++11 parts that combine to form a new way to create software. It enables putting together software using isolated pieces made with existing languages and tools in such a way that iterations are faster, development is more transparent, and the resulting programs effortlessly use all available concurrency. 

The big picture is to create C++11 programs using high level message passing and data flow nodes that communicate using indices into a lock free key value store.  Threads only need to be created once and all take serialized chunks of data from a lock free queue and run them with their destination node. The nodes would be non-trivial native shared libraries that result in a native program without dependencies

The idea came about while I was doing raytraced rendering research. I was creating a kd-tree with arbitrary dimensionality and eventually hit bugs that I felt were too difficult to fix without seeing the kd-tree visually.  The first version paused the program and displayed the kd-tree in an openGL window. The second version was able to use a separate thread to watch the kd-tree being constructed in real time. The third was a separate program that communicated over local IP, then shared memory.  Now I've made my own shared memory key value store (simdb.hpp) that is one file, has 100% lock free concurrency, is cross platform,  Apache 2 licensed and can do roughly 500,000 reads and write per core.

The other aspect came from experience with node based tools such as houdini, nuke and touch designer.  These all have data flow node based workflows, which work exceptionally well in their respective contexts.  With the use of message passing nodes that can send arbitrary data to each other, high level logic and decisions can be introduced elegantly, solving a major Achilles heel of data flow nodes.  To quote Sean Parent "all systems eventually grow and turn into a network problem"  



LAVA is a series of simple, modular, C++11 parts that combine to form a new way to create software. It enables putting together software using isolated pieces made with existing languages and tools in such a way that iterations are faster, development is more transparent, and the resulting programs effortlessly use all available concurrency. 

The big picture is to create C++11 programs using high level message passing and data flow nodes that communicate using indices into a lock free key value store.  Threads only need to be created once and all take serialized chunks of data from a lock free queue and run them with their destination node. The nodes would be non-trivial native shared libraries that result in a native program without dependencies.  

The idea came from raytraced rendering research. I was creating a kd-tree with arbitrary dimensionality and eventually hit bugs that I felt were too difficult to fix without seeing the kd-tree visually.  The first version paused the program and displayed the kd-tree in an openGL window. The second version was able to use a separate thread to watch the kd-tree being constructed in real time. The third was a separate program that communicated over local IP, and later, shared memory.  Now I've written my own shared memory key value store (simdb.hpp) that is one file, has 100% lock free concurrency, is cross platform,  Apache 2 licensed and can do roughly 500,000 reads and write per core.

The other aspect came from experience with node based tools such as houdini, nuke and touch designer.  These all have data flow node based workflows, which work exceptionally well in their respective contexts. Since the inputs and outputs of every node can be visualized, execution can be watched live from any point in the program and problems can be found quickly. Inputs can be frozen so that a single node can be iterated rapidly with the same data passed through and visualized on every change.  With the use of message passing nodes that can send arbitrary data to each other, high level logic and decisions can be introduced elegantly, solving a major Achilles heel of data flow workflows.  To quote Sean Parent "all systems eventually grow and turn into a network problem"  

I think this has enormous potential for both AI and software in general. The ability to create real software that isn't limited to a sandbox or VM while enabling the ability to iterate quickly, build modularly and understand visually is something I think is worth working towards and completely obtainable.  Many of the pieces are already in place to an extent while others exist in a rough form or in previous versions.








LAVA is a series of simple, modular, C++11 parts that combine to form a new way to create software. It enables putting together software using isolated pieces made with existing languages and tools in such a way that iterations are faster, development is more transparent, and the resulting programs effortlessly use all available concurrency. 

The big picture is to create C++11 programs using high level message passing and data flow nodes that communicate using indices into a lock free key value store.  Threads only need to be created once and all take serialized chunks of data from a lock free queue and run them with their destination node. The nodes would be non-trivial native shared libraries that result in a native program without dependencies.  

The idea came from raytraced rendering research. I was creating a kd-tree with arbitrary dimensionality and eventually hit bugs that I felt were too difficult to fix without seeing the kd-tree visually.  The first version paused the program and displayed the kd-tree in an openGL window. The second version was able to use a separate thread to watch the kd-tree being constructed in real time. The third was a separate program that communicated over local IP, and later, shared memory.  Now I've written my own shared memory key value store (simdb.hpp) that is one file, has 100% lock free concurrency, is cross platform,  Apache 2 licensed and can do roughly 500,000 reads and write per core.

The other aspect came from experience with node based tools such as houdini, nuke and touch designer.  These all have data flow node based workflows, which work exceptionally well in their respective contexts. Since the inputs and outputs of every node can be visualized, execution can be watched live from any point in the program and problems can be found quickly. Inputs can be frozen so that a single node can be iterated rapidly with the same data passed through and visualized on every change.  With the use of message passing nodes that can send arbitrary data to each other, high level logic and decisions can be introduced elegantly, solving a major Achilles heel of data flow workflows.  To quote Sean Parent "all systems eventually grow and turn into a network problem"  

I think this has enormous potential for both AI and software in general. The ability to create real software that isn't limited to a sandbox or VM while enabling the ability to iterate quickly, build modularly and understand visually is something I think is worth working towards and completely obtainable.  Many of the pieces are already in place to an extent while others exist in a rough form or in previous versions.

